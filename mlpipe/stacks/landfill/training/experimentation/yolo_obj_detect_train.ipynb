{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landfill Detection with YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.46.1\n",
    "!pip install pytorch-lightning\n",
    "!pip install wandb\n",
    "!pip install roboflow\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install pycocotools\n",
    "!pip install numpy\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataset from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://scrnts-dev-dataplat-ai-models-eu-west-1-772012299168/datasets/landfills-detection/v2/ ./dataset\n",
    "\n",
    "dataset_path = 'dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import os\n",
    "from transformers import YolosImageProcessor\n",
    "\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, feature_extractor, train=True):\n",
    "        ann_file = os.path.join(img_folder, \"_annotations.coco.json\")\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "\n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {\"image_id\": image_id, \"annotations\": target}\n",
    "        encoding = self.feature_extractor(\n",
    "            images=img, annotations=target, return_tensors=\"pt\"\n",
    "        )\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()  # remove batch dimension\n",
    "\n",
    "        target = encoding[\"labels\"][0]  # remove batch dimension\n",
    "\n",
    "        return pixel_values, target\n",
    "\n",
    "\n",
    "feature_extractor = YolosImageProcessor.from_pretrained(\n",
    "    \"hustvl/yolos-small\", size=1000, max_size=1200              # <- SHOULD I USE A DIFFERENT YOLO MODEL? (TODO)\n",
    ")\n",
    "\n",
    "train_dataset = CocoDetection(\n",
    "    img_folder=(dataset_path + \"/train\"), feature_extractor=feature_extractor\n",
    ")\n",
    "val_dataset = CocoDetection(\n",
    "    img_folder=(dataset_path + \"/valid\"),\n",
    "    feature_extractor=feature_extractor,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualize that our data has loaded correctly - You can hit this cell as many times as you want to vizualize how your training set has loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# based on https://github.com/woctezuma/finetune-detr/blob/master/finetune_detr.ipynb\n",
    "image_ids = train_dataset.coco.getImgIds()\n",
    "# let's pick a random image\n",
    "image_id = image_ids[np.random.randint(0, len(image_ids))]\n",
    "print(\"Image nÂ°{}\".format(image_id))\n",
    "image = train_dataset.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join(dataset_path + \"/train\", image[\"file_name\"]))\n",
    "\n",
    "annotations = train_dataset.coco.imgToAnns[image_id]\n",
    "draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "\n",
    "cats = train_dataset.coco.cats\n",
    "id2label = {k: v[\"name\"] for k, v in cats.items()}\n",
    "\n",
    "for annotation in annotations:\n",
    "    box = annotation[\"bbox\"]\n",
    "    class_idx = annotation[\"category_id\"]\n",
    "    x, y, w, h = tuple(box)\n",
    "    draw.rectangle((x, y, x + w, y + h), outline=\"blue\", width=2)\n",
    "    draw.text((x, y), id2label[class_idx], fill=\"white\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = feature_extractor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[1] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, collate_fn=collate_fn, batch_size=1, shuffle=True\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=1)\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup YOLO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import AutoModelForObjectDetection\n",
    "import torch\n",
    "\n",
    "\n",
    "# we wrap our model around pytorch lightning for training\n",
    "class YoloModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, lr, weight_decay):\n",
    "        super().__init__()\n",
    "        # replace COCO classification head with custom head\n",
    "        self.model = AutoModelForObjectDetection.from_pretrained(\n",
    "            \"hustvl/yolos-tiny\", num_labels=len(id2label), ignore_mismatched_sizes=True\n",
    "        )\n",
    "        # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.save_hyperparameters()  # adding this will save the hyperparameters to W&B too\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.model(pixel_values=pixel_values)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "        outputs = self.model(pixel_values=pixel_values, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss_dict = outputs.loss_dict\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\n",
    "            \"train/loss\", loss\n",
    "        )  # logging metrics with a forward slash will ensure the train and validation metrics as split into 2 separate sections in the W&B workspace\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\n",
    "                \"train/\" + k, v.item()\n",
    "            )  # logging metrics with a forward slash will ensure the train and validation metrics as split into 2 separate sections in the W&B workspace\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\n",
    "            \"validation/loss\", loss\n",
    "        )  # logging metrics with a forward slash will ensure the train and validation metrics as split into 2 separate sections in the W&B workspace\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\n",
    "                \"validation/\" + k, v.item()\n",
    "            )  #  logging metrics with a forward slash will ensure the train and validation metrics as split into 2 separate sections in the W&B workspace\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "model = YoloModel(lr=2.5e-5, weight_decay=1e-4)\n",
    "\n",
    "# Keep track of the checkpoint with the lowest validation loss\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"validation/loss\", mode=\"min\")\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# more epochs leads to a tighter fit of your model to your data.\n",
    "trainer = Trainer(\n",
    "    accelerator='auto',\n",
    "    max_epochs=50,\n",
    "    gradient_clip_val=0.1,\n",
    "    accumulate_grad_batches=8,\n",
    "    log_every_n_steps=5,\n",
    "    callbacks=[checkpoint_callback],\n",
    ") \n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir content/deploy\n",
    "!mkdir content/deploy/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile content/deploy/code/inference.py\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    pipe = pipeline(\"object-detection\", model=model_dir, threshold=0.1)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/content/deploy/preprocessor_config.json\", \"w\") as f:\n",
    "    for l in feature_extractor.to_json_string().split(\"\\n\"):\n",
    "        f.write(l + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/deploy/code/inference.py\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    pipe = pipeline(\"object-detection\", model=model_dir, threshold=0.1)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/deploy/code/requirements.txt\n",
    "\n",
    "transformers==4.46.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.model.save_pretrained(\"/content/deploy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Produce Model Version based on current date\n",
    "modelversion = datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "s3_file=f\"model-v{modelversion}.tar.gz\"\n",
    "s3_location=f\"s3://scrnts-dev-dataplat-ai-models-eu-west-1-772012299168/models/landfills/{s3_file}\"\n",
    "%cd /content/deploy/\n",
    "!tar zcvf model.tar.gz *\n",
    "!aws s3 cp model.tar.gz {s3_location}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
